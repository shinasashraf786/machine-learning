{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMdfBvU5I4QohkIVwdHMbQz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"V2MFnp2AQpof"},"outputs":[],"source":["import nltk"]},{"cell_type":"code","source":["help(nltk)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RYFSgmdTQ_Ft","executionInfo":{"status":"ok","timestamp":1717220396835,"user_tz":-330,"elapsed":389,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"2e5e3984-9c23-4e8a-fef6-c0006c3c296f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on package nltk:\n","\n","NAME\n","    nltk\n","\n","DESCRIPTION\n","    The Natural Language Toolkit (NLTK) is an open source Python library\n","    for Natural Language Processing.  A free online book is available.\n","    (If you use the library for academic research, please cite the book.)\n","    \n","    Steven Bird, Ewan Klein, and Edward Loper (2009).\n","    Natural Language Processing with Python.  O'Reilly Media Inc.\n","    https://www.nltk.org/book/\n","    \n","    isort:skip_file\n","    \n","    @version: 3.8.1\n","\n","PACKAGE CONTENTS\n","    app (package)\n","    book\n","    ccg (package)\n","    chat (package)\n","    chunk (package)\n","    classify (package)\n","    cli\n","    cluster (package)\n","    collections\n","    collocations\n","    compat\n","    corpus (package)\n","    data\n","    decorators\n","    downloader\n","    draw (package)\n","    featstruct\n","    grammar\n","    help\n","    inference (package)\n","    internals\n","    jsontags\n","    langnames\n","    lazyimport\n","    lm (package)\n","    metrics (package)\n","    misc (package)\n","    parse (package)\n","    probability\n","    sem (package)\n","    sentiment (package)\n","    stem (package)\n","    tag (package)\n","    tbl (package)\n","    test (package)\n","    text\n","    tgrep\n","    tokenize (package)\n","    toolbox\n","    translate (package)\n","    tree (package)\n","    treeprettyprinter\n","    treetransforms\n","    twitter (package)\n","    util\n","    wsd\n","\n","SUBMODULES\n","    agreement\n","    aline\n","    api\n","    arlstem\n","    arlstem2\n","    association\n","    bleu_score\n","    bllip\n","    boxer\n","    brill\n","    brill_trainer\n","    casual\n","    chart\n","    chrf_score\n","    cistem\n","    confusionmatrix\n","    corenlp\n","    crf\n","    decisiontree\n","    dependencygraph\n","    destructive\n","    discourse\n","    distance\n","    drt\n","    earleychart\n","    evaluate\n","    featurechart\n","    gale_church\n","    gdfa\n","    gleu_score\n","    glue\n","    hmm\n","    hunpos\n","    ibm1\n","    ibm2\n","    ibm3\n","    ibm4\n","    ibm5\n","    ibm_model\n","    isri\n","    lancaster\n","    legality_principle\n","    lfg\n","    linearlogic\n","    logic\n","    mace\n","    malt\n","    mapping\n","    maxent\n","    megam\n","    meteor_score\n","    mwe\n","    naivebayes\n","    nist_score\n","    nonprojectivedependencyparser\n","    paice\n","    pchart\n","    perceptron\n","    phrase_based\n","    porter\n","    positivenaivebayes\n","    projectivedependencyparser\n","    prover9\n","    punkt\n","    recursivedescent\n","    regexp\n","    relextract\n","    repp\n","    resolution\n","    ribes_score\n","    rslp\n","    rte_classify\n","    scikitlearn\n","    scores\n","    segmentation\n","    senna\n","    sequential\n","    sexpr\n","    shiftreduce\n","    simple\n","    snowball\n","    sonority_sequencing\n","    spearman\n","    stack_decoder\n","    stanford\n","    stanford_segmenter\n","    tableau\n","    tadm\n","    textcat\n","    texttiling\n","    tnt\n","    toktok\n","    transitionparser\n","    treebank\n","    viterbi\n","    weka\n","    wordnet\n","\n","FUNCTIONS\n","    demo()\n","        # FIXME:  override any accidentally imported demo, see https://github.com/nltk/nltk/issues/2116\n","    \n","    tee(iterable, n=2, /)\n","        Returns a tuple of n independent iterators.\n","\n","DATA\n","    RUS_PICKLE = 'taggers/averaged_perceptron_tagger_ru/averaged_perceptro...\n","    SLASH = *slash*\n","    TYPE = *type*\n","    __author_email__ = 'nltk.team@gmail.com'\n","    __classifiers__ = ['Development Status :: 5 - Production/Stable', 'Int...\n","    __copyright__ = 'Copyright (C) 2001-2023 NLTK Project.\\n\\nDistribut......\n","    __keywords__ = ['NLP', 'CL', 'natural language processing', 'computati...\n","    __license__ = 'Apache License, Version 2.0'\n","    __longdescr__ = 'The Natural Language Toolkit (NLTK) is a Python ...NL...\n","    __maintainer__ = 'NLTK Team'\n","    __maintainer_email__ = 'nltk.team@gmail.com'\n","    __url__ = 'https://www.nltk.org/'\n","    app = <LazyModule 'nltk.app'>\n","    chat = <LazyModule 'nltk.chat'>\n","    corpus = <LazyModule 'nltk.corpus'>\n","    infile = <_io.TextIOWrapper name='/usr/local/lib/python3....packages/n...\n","    json_tags = {'!nltk.tag.BrillTagger': <class 'nltk.tag.brill.BrillTagg...\n","    toolbox = <LazyModule 'nltk.toolbox'>\n","    version_file = '/usr/local/lib/python3.10/dist-packages/nltk/VERSION'\n","\n","VERSION\n","    3.8.1\n","\n","AUTHOR\n","    NLTK Team\n","\n","FILE\n","    /usr/local/lib/python3.10/dist-packages/nltk/__init__.py\n","\n","\n"]}]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6AxuQc80VQfk","executionInfo":{"status":"ok","timestamp":1717220772609,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"c057df8e-8243-449d-cbb3-18c67715def1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","\n","stopwordss = stopwords.words('english')"],"metadata":{"id":"4XGVD03NRvWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stopwordss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUJSW0y9SBfY","executionInfo":{"status":"ok","timestamp":1717220776323,"user_tz":-330,"elapsed":364,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"4ad14f91-d999-478c-a0bd-9282acb72b27"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i',\n"," 'me',\n"," 'my',\n"," 'myself',\n"," 'we',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'you',\n"," \"you're\",\n"," \"you've\",\n"," \"you'll\",\n"," \"you'd\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves',\n"," 'he',\n"," 'him',\n"," 'his',\n"," 'himself',\n"," 'she',\n"," \"she's\",\n"," 'her',\n"," 'hers',\n"," 'herself',\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'they',\n"," 'them',\n"," 'their',\n"," 'theirs',\n"," 'themselves',\n"," 'what',\n"," 'which',\n"," 'who',\n"," 'whom',\n"," 'this',\n"," 'that',\n"," \"that'll\",\n"," 'these',\n"," 'those',\n"," 'am',\n"," 'is',\n"," 'are',\n"," 'was',\n"," 'were',\n"," 'be',\n"," 'been',\n"," 'being',\n"," 'have',\n"," 'has',\n"," 'had',\n"," 'having',\n"," 'do',\n"," 'does',\n"," 'did',\n"," 'doing',\n"," 'a',\n"," 'an',\n"," 'the',\n"," 'and',\n"," 'but',\n"," 'if',\n"," 'or',\n"," 'because',\n"," 'as',\n"," 'until',\n"," 'while',\n"," 'of',\n"," 'at',\n"," 'by',\n"," 'for',\n"," 'with',\n"," 'about',\n"," 'against',\n"," 'between',\n"," 'into',\n"," 'through',\n"," 'during',\n"," 'before',\n"," 'after',\n"," 'above',\n"," 'below',\n"," 'to',\n"," 'from',\n"," 'up',\n"," 'down',\n"," 'in',\n"," 'out',\n"," 'on',\n"," 'off',\n"," 'over',\n"," 'under',\n"," 'again',\n"," 'further',\n"," 'then',\n"," 'once',\n"," 'here',\n"," 'there',\n"," 'when',\n"," 'where',\n"," 'why',\n"," 'how',\n"," 'all',\n"," 'any',\n"," 'both',\n"," 'each',\n"," 'few',\n"," 'more',\n"," 'most',\n"," 'other',\n"," 'some',\n"," 'such',\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'only',\n"," 'own',\n"," 'same',\n"," 'so',\n"," 'than',\n"," 'too',\n"," 'very',\n"," 's',\n"," 't',\n"," 'can',\n"," 'will',\n"," 'just',\n"," 'don',\n"," \"don't\",\n"," 'should',\n"," \"should've\",\n"," 'now',\n"," 'd',\n"," 'll',\n"," 'm',\n"," 'o',\n"," 're',\n"," 've',\n"," 'y',\n"," 'ain',\n"," 'aren',\n"," \"aren't\",\n"," 'couldn',\n"," \"couldn't\",\n"," 'didn',\n"," \"didn't\",\n"," 'doesn',\n"," \"doesn't\",\n"," 'hadn',\n"," \"hadn't\",\n"," 'hasn',\n"," \"hasn't\",\n"," 'haven',\n"," \"haven't\",\n"," 'isn',\n"," \"isn't\",\n"," 'ma',\n"," 'mightn',\n"," \"mightn't\",\n"," 'mustn',\n"," \"mustn't\",\n"," 'needn',\n"," \"needn't\",\n"," 'shan',\n"," \"shan't\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'wasn',\n"," \"wasn't\",\n"," 'weren',\n"," \"weren't\",\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\"]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJy2bxmiSNwA","executionInfo":{"status":"ok","timestamp":1717221024130,"user_tz":-330,"elapsed":996,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"5dbb8db0-6080-4580-be81-04c92418a313"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["sentence = '\"at eight o clock on thursday morning,hello how are you'\n","\n","tokens = nltk.word_tokenize(sentence)\n","\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FrbTP17HVE7Q","executionInfo":{"status":"ok","timestamp":1717221734104,"user_tz":-330,"elapsed":413,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"2d2971ea-4263-481a-bfd1-11864861518f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['``',\n"," 'at',\n"," 'eight',\n"," 'o',\n"," 'clock',\n"," 'on',\n"," 'thursday',\n"," 'morning',\n"," ',',\n"," 'hello',\n"," 'how',\n"," 'are',\n"," 'you']"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","sentence = '\"at eight o clock on thursday morning,hello how are you'\n","\n","stopwordss = set(stopwords.words('english'))\n","\n","tokens = word_tokenize(sentence)\n","\n","tokens\n","\n","words = [w for w in tokens if not w in stopwordss]\n","\n","words\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q92u3WzpV0R8","executionInfo":{"status":"ok","timestamp":1717221735654,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"c1dc8a5b-f506-4118-d4a1-e3b8d59fa5f8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['``', 'eight', 'clock', 'thursday', 'morning', ',', 'hello']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["\n","\n","sentence = '\"at Eight o Clock on Thursday morning,hello how are you'\n","\n","stopwordss = set(stopwords.words('english'))\n","\n","tokens = word_tokenize(sentence)\n","\n","tokens\n","\n","words = [w.lower() for w in tokens if not w in stopwordss]\n","\n","words\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mgvk6TH3aNcD","executionInfo":{"status":"ok","timestamp":1717221938866,"user_tz":-330,"elapsed":372,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"5bfd0f1e-bc68-434e-b915-28deef8c03ca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['``', 'eight', 'clock', 'thursday', 'morning', ',', 'hello']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["#this is not good\n","\n","#this , is ,not,good\n","\n","\n","\n","\n","#ngrams\n","\n","#ngram=1 ngram=2    ngram=3\n","#this   this is.   this is not\n","\n","\n","#is     is not      is not good\n","\n","#not    not good\n","\n","#good"],"metadata":{"id":"RIbRxEWIcAGe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.util import ngrams\n","\n","sentence = '\"at Eight o Clock on Thursday morning,hello how are you'\n","\n","Ngrams = ngrams(sequence=nltk.word_tokenize(sentence),n=3)\n","\n","\n","for grms in Ngrams:\n","  print(grms)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yn_aNcI4cey9","executionInfo":{"status":"ok","timestamp":1717222435362,"user_tz":-330,"elapsed":372,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"163fc467-e725-460e-b5c5-1ba194fa4b00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('``', 'at', 'Eight')\n","('at', 'Eight', 'o')\n","('Eight', 'o', 'Clock')\n","('o', 'Clock', 'on')\n","('Clock', 'on', 'Thursday')\n","('on', 'Thursday', 'morning')\n","('Thursday', 'morning', ',')\n","('morning', ',', 'hello')\n","(',', 'hello', 'how')\n","('hello', 'how', 'are')\n","('how', 'are', 'you')\n"]}]},{"cell_type":"code","source":["from nltk.stem import SnowballStemmer\n","\n","sb = SnowballStemmer('english')\n","\n","words = ['``', 'eight', 'clock','program','programs','programmer','programming' 'thursday', 'morning', ',', 'hello']\n","\n","\n","for w in words:\n","\n","  print(w,\",\",sb.stem(w))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eWmyovIAhZko","executionInfo":{"status":"ok","timestamp":1717222821539,"user_tz":-330,"elapsed":3,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"e2924419-cac4-4d1a-db3a-cf96bd103c02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["`` , ``\n","eight , eight\n","clock , clock\n","program , program\n","programs , program\n","programmer , programm\n","programmingthursday , programmingthursday\n","morning , morn\n",", , ,\n","hello , hello\n"]}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","porter = PorterStemmer()\n","\n","words = ['``', 'eight', 'clock', 'program', 'programs', 'programmer', 'programming', 'thursday', 'morning', ',', 'hello']\n","\n","print(\"Word, Porter Stem\")\n","for w in words:\n","    print(w, \",\", porter.stem(w))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddZngQNIhbSO","executionInfo":{"status":"ok","timestamp":1717223164612,"user_tz":-330,"elapsed":390,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"151d6352-e38a-4c27-b156-155f3e20fabb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word, Porter Stem\n","`` , ``\n","eight , eight\n","clock , clock\n","program , program\n","programs , program\n","programmer , programm\n","programming , program\n","thursday , thursday\n","morning , morn\n",", , ,\n","hello , hello\n"]}]},{"cell_type":"code","source":["nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qSfKbsOpjZlr","executionInfo":{"status":"ok","timestamp":1717149115207,"user_tz":-330,"elapsed":411,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"d0bb602f-f58f-4a6d-bff6-c75e0beacc9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","\n","lem = WordNetLemmatizer()\n","\n","print('rocks',lem.lemmatize('rocks'))\n","\n","print('thought',lem.lemmatize('thought'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BLG3qIZWkXxu","executionInfo":{"status":"ok","timestamp":1717149119940,"user_tz":-330,"elapsed":2076,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"d6527458-a94a-487d-eae9-c99ca97271a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rocks rock\n","thought thought\n"]}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","\n","lem = WordNetLemmatizer()\n","\n","# Lemmatize as nouns (default behavior)\n","print('rocks as noun:', lem.lemmatize('rocks'))\n","print('thought as noun:', lem.lemmatize('thought'))\n","\n","# Lemmatize as verbs\n","print('rocks as verb:', lem.lemmatize('rocks', pos='v'))\n","print('thought as verb:', lem.lemmatize('thought', pos='v'))\n","print('played as verb:', lem.lemmatize('played', pos='v'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q9U59y91PDMy","executionInfo":{"status":"ok","timestamp":1717068224625,"user_tz":-330,"elapsed":471,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"266622ec-8bfc-4a5c-beec-0c449fd0b7ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rocks as noun: rock\n","thought as noun: thought\n","rocks as verb: rock\n","thought as verb: think\n","played as verb: play\n"]}]},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","\n","lem = WordNetLemmatizer()\n","\n","# Lemmatize words with different POS tags\n","print('rocks as noun:', lem.lemmatize('rocks', pos='n'))  # Noun\n","print('rocks as verb:', lem.lemmatize('rocks', pos='v'))  # Verb\n","print('better as adjective:', lem.lemmatize('better', pos='a'))  # Adjective\n","print('better as adverb:', lem.lemmatize('better', pos='r'))  # Adverb\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MMxrzz96P7hv","executionInfo":{"status":"ok","timestamp":1717068481264,"user_tz":-330,"elapsed":475,"user":{"displayName":"Shinas Ashraf","userId":"02891298623681902961"}},"outputId":"78c5270d-5264-485d-c48e-24f6ca02fbc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rocks as noun: rock\n","rocks as verb: rock\n","better as adjective: good\n","better as adverb: well\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"ueo9zMULQAYc"}},{"cell_type":"markdown","source":["Explanation:\n","lem.lemmatize('rocks', pos='n') lemmatizes 'rocks' as a noun to 'rock'.\n","\n","lem.lemmatize('rocks', pos='v') lemmatizes 'rocks' as a verb to 'rock'.\n","\n","lem.lemmatize('better', pos='a') lemmatizes 'better' as an adjective to\n","'good'.\n","\n","lem.lemmatize('better', pos='r') lemmatizes 'better' as an adverb to 'well'."],"metadata":{"id":"fwPLJuL-QJfz"}}]}